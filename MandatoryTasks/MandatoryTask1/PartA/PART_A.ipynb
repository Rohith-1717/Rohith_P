{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade gensim\n!pip install smart_open==5.2.1\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-19T10:30:46.617603Z","iopub.execute_input":"2025-10-19T10:30:46.617795Z","iopub.status.idle":"2025-10-19T10:30:53.200091Z","shell.execute_reply.started":"2025-10-19T10:30:46.617779Z","shell.execute_reply":"2025-10-19T10:30:53.199039Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.4.0)\nRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\nRequirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.15.3)\nRequirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (5.2.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->gensim) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->gensim) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->gensim) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->gensim) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->gensim) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->gensim) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->gensim) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->gensim) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.5->gensim) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.18.5->gensim) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.18.5->gensim) (2024.2.0)\nRequirement already satisfied: smart_open==5.2.1 in /usr/local/lib/python3.11/dist-packages (5.2.1)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom gensim.models import Word2Vec, FastText\nfrom gensim.models.callbacks import CallbackAny2Vec\nimport sys, time\nfrom csv import writer\n\nclass Progress(CallbackAny2Vec):\n    def __init__(self, name, total):\n        self.name = name\n        self.total = total\n        self.epoch = 0\n        self.start = time.time()\n    def on_epoch_begin(self, model):\n        self.epoch += 1\n        print(f\"\\n{self.name} → Epoch {self.epoch}/{self.total} started...\")\n        self.epoch_time = time.time()\n    def on_epoch_end(self, model):\n        e = time.time() - self.epoch_time\n        t = time.time() - self.start\n        sys.stdout.write(f\"\\r{self.name} → Epoch {self.epoch}/{self.total} completed ({e:.1f}s, total {t:.1f}s)\")\n        sys.stdout.flush()\n    def on_train_end(self, model):\n        print(f\"\\n{self.name} training complete!\\n\")\n\ndata = load_dataset(\"lucadiliello/newsqa\")\ndocs = []\nfor part in ['train', 'validation']:\n    for item in data[part]:\n        docs.append(str(item['context']))\n        docs.append(str(item['question']))\n        for ans in item['answers']:\n            if isinstance(ans, list):\n                for a in ans:\n                    docs.append(str(a))\n            else:\n                docs.append(str(ans))\n\ndef clean(txt):\n    txt = txt.lower()\n    txt = re.sub(r'[^a-z0-9\\s]', '', txt)\n    return txt\n\ndocs = [clean(t) for t in docs]\ntokens = [word_tokenize(t) for t in docs]\n\nepochs = 5\nprint(\"\\nTraining Word2Vec...\")\nw2v = Word2Vec(sentences=tokens, vector_size=150, window=4, min_count=2, workers=4, sg=1, epochs=epochs, callbacks=[Progress(\"Word2Vec\", epochs)])\nwith open(\"w2v_embeddings.csv\", \"w\", newline=\"\") as f:\n    w = writer(f)\n    for i, word in enumerate(w2v.wv.index_to_key):\n        w.writerow([word]+w2v.wv[word].tolist())\n        if i % max(1, len(w2v.wv)//100) == 0:\n            sys.stdout.write(f\"\\rWord2Vec saving progress: {int(i/len(w2v.wv)*100)}%\")\n            sys.stdout.flush()\n    print(\"\\rWord2Vec saving progress: 100%\")\n\nprint(\"\\nTraining FastText...\")\nft = FastText(sentences=tokens, vector_size=120, window=6, min_count=1, workers=4, epochs=epochs, callbacks=[Progress(\"FastText\", epochs)])\nwith open(\"fasttext_embeddings.csv\", \"w\", newline=\"\") as f:\n    w = writer(f)\n    for i, word in enumerate(ft.wv.index_to_key):\n        w.writerow([word]+ft.wv[word].tolist())\n        if i % max(1, len(ft.wv)//100) == 0:\n            sys.stdout.write(f\"\\rFastText saving progress: {int(i/len(ft.wv)*100)}%\")\n            sys.stdout.flush()\n    print(\"\\rFastText saving progress: 100%\")\n\nprint(\"\\nAll embeddings saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T10:30:53.916908Z","iopub.execute_input":"2025-10-19T10:30:53.917211Z","iopub.status.idle":"2025-10-19T10:58:17.692885Z","shell.execute_reply.started":"2025-10-19T10:30:53.917183Z","shell.execute_reply":"2025-10-19T10:58:17.692127Z"}},"outputs":[{"name":"stdout","text":"\nTraining Word2Vec...\n\nWord2Vec → Epoch 1/5 started...\nWord2Vec → Epoch 1/5 completed (110.1s, total 116.5s)\nWord2Vec → Epoch 2/5 started...\nWord2Vec → Epoch 2/5 completed (109.4s, total 225.9s)\nWord2Vec → Epoch 3/5 started...\nWord2Vec → Epoch 3/5 completed (109.9s, total 335.8s)\nWord2Vec → Epoch 4/5 started...\nWord2Vec → Epoch 4/5 completed (110.2s, total 446.0s)\nWord2Vec → Epoch 5/5 started...\nWord2Vec → Epoch 5/5 completed (110.1s, total 556.2s)\nWord2Vec training complete!\n\nWord2Vec saving progress: 100%\n\nTraining FastText...\n\nFastText → Epoch 1/5 started...\nFastText → Epoch 1/5 completed (177.1s, total 189.4s)\nFastText → Epoch 2/5 started...\nFastText → Epoch 2/5 completed (177.8s, total 367.2s)\nFastText → Epoch 3/5 started...\nFastText → Epoch 3/5 completed (167.8s, total 535.0s)\nFastText → Epoch 4/5 started...\nFastText → Epoch 4/5 completed (167.6s, total 702.6s)\nFastText → Epoch 5/5 started...\nFastText → Epoch 5/5 completed (168.7s, total 871.3s)\nFastText training complete!\n\nFastText saving progress: 100%\n\nAll embeddings saved successfully!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"words_to_test = [\"president\", \"government\", \"war\", \"peace\", \"city\", \"village\"]\n\nprint(\"Word2Vec evaluation:\")\nfor w in words_to_test:\n    if w in w2v.wv:\n        sim = w2v.wv.most_similar(w, topn=5)\n        print(f\"\\nTop words similar to '{w}':\")\n        for s, score in sim:\n            print(f\"{s} → {score:.3f}\")\n    else:\n        print(f\"{w} not in Word2Vec vocab\")\n\nprint(\"\\nFastText evaluation:\")\nfor w in words_to_test:\n    if w in ft.wv:\n        sim = ft.wv.most_similar(w, topn=5)\n        print(f\"\\nTop words similar to '{w}':\")\n        for s, score in sim:\n            print(f\"{s} → {score:.3f}\")\n    else:\n        print(f\"{w} not in FastText vocab\")\n\nprint(\"\\nExample word similarity scores:\")\npairs = [(\"king\",\"queen\"), (\"man\",\"woman\"), (\"city\",\"village\"), (\"war\",\"peace\")]\nfor a,b in pairs:\n    if a in w2v.wv and b in w2v.wv:\n        score = w2v.wv.similarity(a,b)\n        print(f\"Word2Vec similarity({a},{b}) = {score:.3f}\")\n    if a in ft.wv and b in ft.wv:\n        score = ft.wv.similarity(a,b)\n        print(f\"FastText similarity({a},{b}) = {score:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T11:04:15.671235Z","iopub.execute_input":"2025-10-19T11:04:15.672237Z","iopub.status.idle":"2025-10-19T11:04:15.822475Z","shell.execute_reply.started":"2025-10-19T11:04:15.672209Z","shell.execute_reply":"2025-10-19T11:04:15.821616Z"}},"outputs":[{"name":"stdout","text":"Word2Vec evaluation:\n\nTop words similar to 'president':\nbarack → 0.676\nobama → 0.665\nexboss → 0.660\nsjon → 0.660\negyptain → 0.658\n\nTop words similar to 'government':\narabdominated → 0.675\nbackpedal → 0.665\nsunnibacked → 0.658\nethiopianbacked → 0.656\nfatahhamas → 0.653\n\nTop words similar to 'war':\niraniraq → 0.703\n19501953 → 0.642\nlongplanned → 0.637\nromuald → 0.636\nii → 0.632\n\nTop words similar to 'peace':\nnaypidaw → 0.662\nbedeviled → 0.649\nnobel → 0.632\ncorecipient → 0.625\negyptianisraeli → 0.621\n\nTop words similar to 'city':\nmoreheadbeaufort → 0.621\napia → 0.609\ncabanatuan → 0.607\nmilehigh → 0.596\ngrenoble → 0.595\n\nTop words similar to 'village':\nokayama → 0.641\nsharqi → 0.631\nkaniguran → 0.618\nhayagay → 0.612\ngalachipa → 0.612\n\nFastText evaluation:\n\nTop words similar to 'president':\nnowpresident → 0.957\ncopresident → 0.956\npresidente → 0.949\npresidenta → 0.943\ntechpresident → 0.939\n\nTop words similar to 'government':\ngovernmenttogovernment → 0.988\nusgovernment → 0.967\nprogovernment → 0.960\ngovernmentloan → 0.958\nbiggovernment → 0.953\n\nTop words similar to 'war':\nwarp → 0.845\nahwar → 0.838\ntugofwar → 0.817\nwarg → 0.807\nwarap → 0.769\n\nTop words similar to 'peace':\npeacesign → 0.850\npeacetime → 0.823\npeaceprize → 0.813\npeaces → 0.803\ngreenpeace → 0.800\n\nTop words similar to 'city':\ncityy → 0.952\ncitymy → 0.891\n29city → 0.842\ncityjabaliya → 0.840\n23city → 0.829\n\nTop words similar to 'village':\nvillagey → 0.950\nevillage → 0.948\necovillage → 0.929\nvillager → 0.878\nvillagelike → 0.875\n\nExample word similarity scores:\nWord2Vec similarity(king,queen) = 0.357\nFastText similarity(king,queen) = 0.354\nWord2Vec similarity(man,woman) = 0.746\nFastText similarity(man,woman) = 0.792\nWord2Vec similarity(city,village) = 0.297\nFastText similarity(city,village) = 0.608\nWord2Vec similarity(war,peace) = 0.310\nFastText similarity(war,peace) = 0.305\n","output_type":"stream"}],"execution_count":3}]}